{
  "hash": "4810888840c0e08a313b0b9e216620b8",
  "result": {
    "markdown": "---\ntitle: \"Assignment 1: Generative Art\"\nauthor: \"Bel Khuu\"\ndate: \"2022-09-12\"\ncategories: [Code, R, Art, Assignment]\nimage: \"image.jpg\"\ndraft: false\nfreeze: true\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\n\n## 1. Generative Art\n\n### Source 1: AIArtist.org - Generative Art Guide\n\nThe [Generative Art Guide](https://aiartists.org/generative-art-design) describes generative art as creating art (shape, forms, colors, etc.) from an computer using a set of rules.\n\nThe page introduces us a various artists who create generative art and algorithm art.\n\n@fig-sprawl is a piece by Mark J. Stock, who tread the line between what's natural and what's computerized.\n\n::: {#fig-sprawl}\n![](https://images.squarespace-cdn.com/content/v1/5c77350965a707ed1710a1bc/1588698303774-0O4IBACSMA5J5ZB00ZOC/Sprawl+Generative+Art+by+Mark+J.+Stock.jpeg?format=1000w%22){width=\"60%\"}\n\nSprawl by Mark J. Stock\n:::\n\nAI generated art is a specific type of generative art to uses machine learning tools to examine existing work. The following source will discuss those a little more.\n\n### Source 2: Tate art Museum, UK - Generative Art\n\nThe [Tate art museum](https://www.tate.org.uk/) adds that generative art usually includes an element of chance. According to the museum, Harold Cohen was considered one of the pioneers for generative art, which started from the [dada](https://www.tate.org.uk/art/art-terms/d/dada) movement.\n\n::: {#fig-tate}\n![](https://media.tate.org.uk/aztate-prd-ew-dg-wgtail-st1-ctr-data/images/.width-1200_UCL4WkH.jpg){width=\"60%\"}\n\nUntitle Computer Drawing by Harold Cohen (1982)\n:::\n\n### Source 3: NYTimes article - Théâtre D'opéra Spatial\n\nThere is no surprise that this new way of creating will leave \"traditional\" artists somewhat uneasy. In this [New York Times](https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html) article, published a few days ago, an A.I. generated piece @fig-allen by artist Jason Allen won this year's the Colorado State Fair's annual art competition.\n\n::: {#fig-allen}\n![](https://static01.nyt.com/images/2022/09/01/business/00roose-1/merlin_212276709_3104aef5-3dc4-4288-bb44-9e5624db0b37-superJumbo.jpg?quality=75&auto=webp%22){width=\"70%\"}\n\nThéâtre D'opéra Spatial by Jason Allen via Midjourney\n:::\n\nAllen explained that he didn't break the rules to the win the Colorado State Fair's annual art competition.\n\nHere is a glimpse of how powerful A.I.-generating tools like [Midjourney](https://www.midjourney.com/home/) or [Dall-E 2](https://openai.com/dall-e-2/) can be.\n\n[KarenXCheng](https://www.instagram.com/karenxcheng/?hl=en) is an video creator uses [Dall-E 2](https://openai.com/dall-e-2/) to create different images for her projects. We can see that in @fig-karen, the tool needs specific prompts to generate different images. The artists, like Cheng, will have to comb though them to select the ones she likes best!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"vembedr\")\nembed_url(\"https://youtu.be/eUSx4_S6O34\")%>%\n  use_align(\"center\")\n```\n\n::: {#fig-karen .cell-output-display}\n```{=html}\n<div class=\"vembedr\" align=\"center\">\n<div>\n<iframe src=\"https://www.youtube.com/embed/eUSx4_S6O34\" width=\"533\" height=\"300\" frameborder=\"0\" allowfullscreen=\"\" data-external=\"1\"></iframe>\n</div>\n</div>\n```\n\n\nKarenXCheng uses Dall-E 2, an AI tool, to generate images.\n:::\n:::\n\n\nIn @fig-karen2, Cheng also uses multiple AI tools including [Dall-E 2](https://openai.com/dall-e-2/) to generate images, and [EbSynth](https://ebsynth.com/) and [DAIN](https://grisk.itch.io/dain-app) to optimize her project.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"vembedr\")\nembed_url(\"https://youtu.be/6-L4vHmt1Dc\")%>%\n  use_align(\"center\")\n```\n\n::: {#fig-karen2 .cell-output-display}\n```{=html}\n<div class=\"vembedr\" align=\"center\">\n<div>\n<iframe src=\"https://www.youtube.com/embed/6-L4vHmt1Dc\" width=\"533\" height=\"300\" frameborder=\"0\" allowfullscreen=\"\" data-external=\"1\"></iframe>\n</div>\n</div>\n```\n\n\nKarenXCheng uses Dall-E 2 and EbSynth to create her project.\n:::\n:::\n\n\n## 2. Fall Color\n\n@fig-lightgreen is a modified version of \"Fall Color,\" using the color \"lightgreen\" instead of \"burlywood3.\"\n\nCredit: <https://fronkonstin.com>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install packages\n\n# install.packages(\"gsubfn\")\n# install.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %>% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %>% rbind(points)->points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %>%\n      rbind(status) -> status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n  \n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]->points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %>%\n      rbind(points) -> points\n    status[-1,]->status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"lightgreen\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n```\n\n::: {.cell-output-display}\n![Light Green Color](index_files/figure-html/fig-lightgreen-1.png){#fig-lightgreen width=672}\n:::\n:::\n\n\n## 3. Critique on a chart from published work\n\n[The Economic Consequences of Increasing Sleep Among the Urban Poor](https://www.nber.org/system/files/working_papers/w26746/w26746.pdf) is an article that discussed an experimental research about sleep and its impact. @fig-sleep sums up the findings of the study.\n\n::: {#fig-sleep}\n![](sleeptreatmentsum.PNG){width=\"70%\"}\n\nSummary of Treatment Effects\n:::\n\n-   Color:\n\nWhen I looked at @fig-sleep, I was immediately drawn to the larger colored area at the bottom of the graphic, separating the effects of a night-sleep treatment and nap treatment. Whatever is being tested, naps has a larger effect than night-sleep.\n\nThe next elements I saw were the tests grouped by colors since my eyes were already focused on the lower part of the graphics.\n\n-   Font:\n\nTo find out what the colors mean, I look at that top of the graphic. The categories of the tests are capitalized a bolted. At this point, I was able to compare the magnitude of the effects between night-sleep and naps. The graphic did a good job guiding my attention.\n\nThe title is in serif, whereas the rest of the graphic has sans serif font.\n\n-   Axis:\n\n    -   The vertical Axis has the size of the effect and horizontal axis has treatments grouped by categories in different colors.\n\n-   Canvas and Additional design:\n\nThe bar in the middle of the rectangles are standard deviations. This a clean and effective way to provide more information about the results.\n\nNext, I was curious about whether these effects were positive or negative. Here is where I don't think the author did the best job. It was easier to see where the baseline (zero effect) was in the nap section thanks to the large rectangles for positive effects. However, it is harder to see whether the effects are positive or negative for night-sleep, especially when they are small like cognition effects. An easy way to rectify this is to add a grey or faint line at 0.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/vembedr-0.1.5/css/vembedr.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}